{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef2861f",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1be186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization, Dropout, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.errors import ResourceExhaustedError\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def calc_class_weights(train_iterator):\n",
    "    \"\"\"\n",
    "    Calculate class weighs dictionary to use as input for the cnn training. This is useful if the training set is\n",
    "    imbalanced.\n",
    "\n",
    "    The weight of class \"i\" is calculated as the number of samples in the most populated class divided by the number of\n",
    "    samples in class i (max_class_frequency / class_frequency).\n",
    "    Note that the class weights are capped at 10. This is done in order to avoid placing too much weight on\n",
    "    small fraction of the dataset. For the same reason, the weight is set to 1 for any class in the training set that\n",
    "    contains fewer than 5 samples.\n",
    "\n",
    "    :param class_counts: A list with the number of files for each class.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Fixed parameters\n",
    "    class_counts = np.unique(train_iterator.classes, return_counts=True)\n",
    "    class_weights = []\n",
    "    max_freq = max(class_counts[1])\n",
    "    class_weights = [max_freq / count for count in class_counts[1]]\n",
    "    \n",
    "    print(\"Classes: \" + str(class_counts[0]))\n",
    "    print(\"Samples per class: \" + str(class_counts[1]))\n",
    "    print(\"Class weights: \" + str(class_weights))\n",
    "\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "def unfreeze_layers(model, last_fixed_layer):\n",
    "    # Retrieve the index of the last fixed layer and add 1 so that it is also set to not trainable\n",
    "    first_trainable = model.layers.index(model.get_layer(last_fixed_layer)) + 1\n",
    "\n",
    "    # Set which layers are trainable.\n",
    "    for layer_idx, layer in enumerate(model.layers):\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = layer_idx >= first_trainable\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(optimiser, last_fixed_layer):\n",
    "    model = EfficientNetB4(include_top=False, input_shape=(300, 400, 3), weights=\"imagenet\")\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = False\n",
    "\n",
    "    # Rebuild top\n",
    "    x = GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = Dense(6, activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "    model = unfreeze_layers(model, last_fixed_layer)\n",
    "    \n",
    "    # Compile\n",
    "    model = Model(model.input, outputs, name=\"EfficientNet\")\n",
    "    model.compile(\n",
    "        optimizer=optimiser, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(rotation, shear, zoom, brightness, lr, last_fixed_layer, batch_size, idx):\n",
    "    model_name = f'efficientnetb4_{idx}'\n",
    "    if os.path.exists(Path('.') / (model_name + '.h5')):\n",
    "        print(f'{model_name} already trained')\n",
    "        return\n",
    "    print(f'Now training {model_name}')\n",
    "    \n",
    "    train_generator = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rotation_range=rotation,\n",
    "        shear_range=shear,\n",
    "        zoom_range=zoom,\n",
    "        brightness_range=brightness,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=preprocess_input,\n",
    "    )\n",
    "    train_iterator = train_generator.flow_from_directory(\n",
    "        '/home/ubuntu/store/internal-neurips/full/train', \n",
    "        target_size=(300, 400),\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        follow_links=True,\n",
    "        interpolation='bilinear',\n",
    "    )\n",
    "    loss_weights = calc_class_weights(train_iterator)\n",
    "\n",
    "    optimiser = Adam(lr=lr)\n",
    "    model = build_model(optimiser, last_fixed_layer)\n",
    "    \n",
    "    logger = CSVLogger(model_name + '.csv')\n",
    "\n",
    "    model.fit(\n",
    "        x=train_iterator,\n",
    "        batch_size=batch_size,\n",
    "        epochs=20,\n",
    "        verbose=True,\n",
    "        class_weight=dict(zip(range(6), loss_weights)),\n",
    "        workers=8,\n",
    "        callbacks=[logger]\n",
    "    )\n",
    "    model.save(model_name + '.h5')\n",
    "\n",
    "\n",
    "for idx in range(5):\n",
    "    train_model(15, 0.5, 0.5, [0.5, 1], 0.001, 'block6d_add', 64, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025cd02",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46b1d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now validating efficientnetb4_4\n",
      "Found 566 images belonging to 6 classes.\n",
      "Now validating efficientnetb4_0\n",
      "Found 566 images belonging to 6 classes.\n",
      "Now validating efficientnetb4_2\n",
      "Found 566 images belonging to 6 classes.\n",
      "Now validating efficientnetb4_3\n",
      "Found 566 images belonging to 6 classes.\n",
      "Now validating efficientnetb4_1\n",
      "Found 566 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.errors import ResourceExhaustedError\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "base_path = \"/home/ubuntu/store/efficientnet-final-size\"\n",
    "model_names = glob.glob(\"/home/ubuntu/store/efficientnet-final-size/*.h5\")\n",
    "\n",
    "for model_path in model_names:\n",
    "    model_name = Path(model_path).stem\n",
    "    if os.path.exists(Path(base_path) / (model_name + '_preds.csv')):\n",
    "        print(f'{model_name} already validated')\n",
    "        continue\n",
    "    print('Now validating', model_name)\n",
    "    valid_generator = ImageDataGenerator(\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=preprocess_input\n",
    "    )\n",
    "    valid_iterator = valid_generator.flow_from_directory(\n",
    "        '/home/ubuntu/store/DermX-test-set/test', \n",
    "        batch_size=8, \n",
    "        target_size=(300, 400),\n",
    "        class_mode='categorical',\n",
    "        follow_links=True,\n",
    "        interpolation='bilinear',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    model = load_model(Path(model_path))\n",
    "    preds = [np.argmax(pred) for pred in model.predict(valid_iterator)]\n",
    "    actual = valid_iterator.labels\n",
    "    preds_df = pd.DataFrame.from_dict({'actual': actual, 'pred': preds, 'filenames': valid_iterator.filenames}).to_pickle(Path(base_path) / (model_name + '_preds.csv'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce60442",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde0e310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>efficientnetb4_3_preds</th>\n",
       "      <td>0.460444</td>\n",
       "      <td>0.460606</td>\n",
       "      <td>0.436414</td>\n",
       "      <td>566</td>\n",
       "      <td>0.485866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficientnetb4_2_preds</th>\n",
       "      <td>0.450772</td>\n",
       "      <td>0.451591</td>\n",
       "      <td>0.442826</td>\n",
       "      <td>566</td>\n",
       "      <td>0.477032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficientnetb4_1_preds</th>\n",
       "      <td>0.434676</td>\n",
       "      <td>0.456591</td>\n",
       "      <td>0.438718</td>\n",
       "      <td>566</td>\n",
       "      <td>0.482332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficientnetb4_0_preds</th>\n",
       "      <td>0.468710</td>\n",
       "      <td>0.404031</td>\n",
       "      <td>0.382892</td>\n",
       "      <td>566</td>\n",
       "      <td>0.427562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficientnetb4_4_preds</th>\n",
       "      <td>0.373043</td>\n",
       "      <td>0.224015</td>\n",
       "      <td>0.153004</td>\n",
       "      <td>566</td>\n",
       "      <td>0.236749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        precision    recall  f1-score  support  accuracy\n",
       "efficientnetb4_3_preds   0.460444  0.460606  0.436414      566  0.485866\n",
       "efficientnetb4_2_preds   0.450772  0.451591  0.442826      566  0.477032\n",
       "efficientnetb4_1_preds   0.434676  0.456591  0.438718      566  0.482332\n",
       "efficientnetb4_0_preds   0.468710  0.404031  0.382892      566  0.427562\n",
       "efficientnetb4_4_preds   0.373043  0.224015  0.153004      566  0.236749"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "base_path = \"/home/ubuntu/store/efficientnet-final-size\"\n",
    "model_preds = glob.glob(\"/home/ubuntu/store/efficientnet-final-size/*_preds.csv\")\n",
    "model_comparison_dict = {}\n",
    "\n",
    "for model_pred in model_preds:\n",
    "    model_preds_df = pd.read_pickle(Path(model_pred))\n",
    "    model_comparison_dict[Path(model_pred).stem] = classification_report(\n",
    "        model_preds_df['actual'], \n",
    "        model_preds_df['pred'],\n",
    "        labels=[0, 1, 2, 3, 4, 5],\n",
    "        target_names=['acne', 'actinic_keratosis', 'psoriasis_no_pustular', 'seborrheic_dermatitis', 'vitiligo', 'wart'],\n",
    "        output_dict=True\n",
    "    )['macro avg']\n",
    "    model_comparison_dict[Path(model_pred).stem]['accuracy'] = len(model_preds_df[model_preds_df['actual'] == model_preds_df['pred']]) / len(model_preds_df)\n",
    "    \n",
    "model_comparison_df = pd.DataFrame.from_dict(model_comparison_dict, orient='index')\n",
    "model_comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13bc33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_3",
   "language": "python",
   "name": "tf2_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
