{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1be186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training efficientnetb4_0\n",
      "Found 3214 images belonging to 6 classes.\n",
      "Classes: [0 1 2 3 4 5]\n",
      "Samples per class: [1177  165  975  113  178  606]\n",
      "Class weights: [1.0, 7.133333333333334, 1.2071794871794872, 10.415929203539823, 6.612359550561798, 1.9422442244224423]\n",
      "Epoch 1/20\n",
      "51/51 [==============================] - 29s 572ms/step - loss: 3.7468 - accuracy: 0.4695\n",
      "Epoch 2/20\n",
      "51/51 [==============================] - 30s 597ms/step - loss: 3.1394 - accuracy: 0.5691\n",
      "Epoch 3/20\n",
      "51/51 [==============================] - 30s 590ms/step - loss: 2.7079 - accuracy: 0.5924\n",
      "Epoch 4/20\n",
      "51/51 [==============================] - 30s 595ms/step - loss: 1.6125 - accuracy: 0.7442\n",
      "Epoch 7/20\n",
      "51/51 [==============================] - 30s 588ms/step - loss: 1.5664 - accuracy: 0.7480\n",
      "Epoch 8/20\n",
      "51/51 [==============================] - 30s 591ms/step - loss: 1.4884 - accuracy: 0.7604\n",
      "Epoch 9/20\n",
      "51/51 [==============================] - 31s 601ms/step - loss: 1.4023 - accuracy: 0.7757\n",
      "Epoch 10/20\n",
      "51/51 [==============================] - 30s 591ms/step - loss: 1.7244 - accuracy: 0.7701\n",
      "Epoch 11/20\n",
      "51/51 [==============================] - 30s 594ms/step - loss: 1.3999 - accuracy: 0.7716\n",
      "Epoch 12/20\n",
      "51/51 [==============================] - 30s 591ms/step - loss: 1.0876 - accuracy: 0.8248\n",
      "Epoch 13/20\n",
      "51/51 [==============================] - 30s 593ms/step - loss: 0.8664 - accuracy: 0.8276\n",
      "Epoch 14/20\n",
      "51/51 [==============================] - 30s 592ms/step - loss: 0.7996 - accuracy: 0.8597\n",
      "Epoch 15/20\n",
      "51/51 [==============================] - 30s 593ms/step - loss: 0.8833 - accuracy: 0.8444\n",
      "Epoch 16/20\n",
      "51/51 [==============================] - 30s 597ms/step - loss: 0.6689 - accuracy: 0.8777\n",
      "Epoch 17/20\n",
      "51/51 [==============================] - 30s 594ms/step - loss: 0.5266 - accuracy: 0.8951\n",
      "Epoch 18/20\n",
      "51/51 [==============================] - 31s 605ms/step - loss: 0.4734 - accuracy: 0.9063\n",
      "Epoch 19/20\n",
      "51/51 [==============================] - 30s 591ms/step - loss: 0.5096 - accuracy: 0.9039\n",
      "Epoch 20/20\n",
      "51/51 [==============================] - 30s 592ms/step - loss: 0.6805 - accuracy: 0.8783\n",
      "Now training efficientnetb4_1\n",
      "Found 3214 images belonging to 6 classes.\n",
      "Classes: [0 1 2 3 4 5]\n",
      "Samples per class: [1177  165  975  113  178  606]\n",
      "Class weights: [1.0, 7.133333333333334, 1.2071794871794872, 10.415929203539823, 6.612359550561798, 1.9422442244224423]\n",
      "Epoch 1/20\n",
      "51/51 [==============================] - 30s 589ms/step - loss: 3.8541 - accuracy: 0.4658\n",
      "Epoch 2/20\n",
      "51/51 [==============================] - 30s 590ms/step - loss: 2.6607 - accuracy: 0.6117\n",
      "Epoch 3/20\n",
      "51/51 [==============================] - 30s 592ms/step - loss: 1.9942 - accuracy: 0.6686\n",
      "Epoch 4/20\n",
      "51/51 [==============================] - 30s 590ms/step - loss: 1.6548 - accuracy: 0.7240\n",
      "Epoch 5/20\n",
      "51/51 [==============================] - 31s 601ms/step - loss: 1.6524 - accuracy: 0.7274\n",
      "Epoch 6/20\n",
      "51/51 [==============================] - 31s 598ms/step - loss: 1.5068 - accuracy: 0.7362\n",
      "Epoch 7/20\n",
      "51/51 [==============================] - 30s 591ms/step - loss: 1.3574 - accuracy: 0.7657\n",
      "Epoch 8/20\n",
      "51/51 [==============================] - 30s 588ms/step - loss: 1.2565 - accuracy: 0.8130\n",
      "Epoch 9/20\n",
      "51/51 [==============================] - 30s 591ms/step - loss: 1.0945 - accuracy: 0.8139\n",
      "Epoch 10/20\n",
      "51/51 [==============================] - 30s 590ms/step - loss: 0.8273 - accuracy: 0.8594\n",
      "Epoch 11/20\n",
      "51/51 [==============================] - 30s 596ms/step - loss: 0.6773 - accuracy: 0.8665\n",
      "Epoch 12/20\n",
      "51/51 [==============================] - 30s 597ms/step - loss: 0.6695 - accuracy: 0.8768\n",
      "Epoch 13/20\n",
      "51/51 [==============================] - 30s 598ms/step - loss: 0.5781 - accuracy: 0.8955\n",
      "Epoch 14/20\n",
      "51/51 [==============================] - 31s 605ms/step - loss: 0.6427 - accuracy: 0.8827\n",
      "Epoch 15/20\n",
      "51/51 [==============================] - 30s 593ms/step - loss: 0.6261 - accuracy: 0.8818\n",
      "Epoch 16/20\n",
      "51/51 [==============================] - 30s 595ms/step - loss: 0.5502 - accuracy: 0.8986\n",
      "Epoch 17/20\n",
      "51/51 [==============================] - 30s 591ms/step - loss: 0.5211 - accuracy: 0.9076\n",
      "Epoch 18/20\n",
      "51/51 [==============================] - 30s 584ms/step - loss: 0.3924 - accuracy: 0.9228\n",
      "Epoch 19/20\n",
      "51/51 [==============================] - 30s 597ms/step - loss: 0.5096 - accuracy: 0.9101\n",
      "Epoch 20/20\n",
      "51/51 [==============================] - 30s 593ms/step - loss: 0.4901 - accuracy: 0.9042\n",
      "Now training efficientnetb4_2\n",
      "Found 3214 images belonging to 6 classes.\n",
      "Classes: [0 1 2 3 4 5]\n",
      "Samples per class: [1177  165  975  113  178  606]\n",
      "Class weights: [1.0, 7.133333333333334, 1.2071794871794872, 10.415929203539823, 6.612359550561798, 1.9422442244224423]\n",
      "Epoch 1/20\n",
      "51/51 [==============================] - 30s 594ms/step - loss: 3.7059 - accuracy: 0.4396\n",
      "Epoch 2/20\n",
      "51/51 [==============================] - 30s 593ms/step - loss: 2.8619 - accuracy: 0.5638\n",
      "Epoch 3/20\n",
      "51/51 [==============================] - 30s 595ms/step - loss: 2.2738 - accuracy: 0.6344\n",
      "Epoch 4/20\n",
      "51/51 [==============================] - 30s 594ms/step - loss: 1.8572 - accuracy: 0.6942\n",
      "Epoch 5/20\n",
      "51/51 [==============================] - 30s 597ms/step - loss: 1.5941 - accuracy: 0.7408\n",
      "Epoch 6/20\n",
      "51/51 [==============================] - 31s 599ms/step - loss: 1.6356 - accuracy: 0.7334\n",
      "Epoch 7/20\n",
      "51/51 [==============================] - 31s 604ms/step - loss: 1.3791 - accuracy: 0.7673\n",
      "Epoch 8/20\n",
      "51/51 [==============================] - 31s 603ms/step - loss: 1.1216 - accuracy: 0.8034\n",
      "Epoch 9/20\n",
      "51/51 [==============================] - 31s 600ms/step - loss: 0.9604 - accuracy: 0.8370\n",
      "Epoch 10/20\n",
      "51/51 [==============================] - 30s 588ms/step - loss: 0.8311 - accuracy: 0.8457\n",
      "Epoch 11/20\n",
      "51/51 [==============================] - 30s 588ms/step - loss: 0.7156 - accuracy: 0.8653\n",
      "Epoch 12/20\n",
      "51/51 [==============================] - 30s 592ms/step - loss: 0.6228 - accuracy: 0.8821\n",
      "Epoch 13/20\n",
      "51/51 [==============================] - 30s 596ms/step - loss: 0.7204 - accuracy: 0.8752\n",
      "Epoch 14/20\n",
      "51/51 [==============================] - 30s 596ms/step - loss: 0.4835 - accuracy: 0.9070\n",
      "Epoch 15/20\n",
      "51/51 [==============================] - 30s 593ms/step - loss: 0.5505 - accuracy: 0.9007\n",
      "Epoch 16/20\n",
      "51/51 [==============================] - 30s 591ms/step - loss: 0.5814 - accuracy: 0.9029\n",
      "Epoch 17/20\n",
      "51/51 [==============================] - 30s 588ms/step - loss: 0.5166 - accuracy: 0.9029\n",
      "Epoch 18/20\n",
      "51/51 [==============================] - 30s 594ms/step - loss: 0.5297 - accuracy: 0.9020\n",
      "Epoch 19/20\n",
      "51/51 [==============================] - 30s 592ms/step - loss: 0.5526 - accuracy: 0.8989\n",
      "Epoch 20/20\n",
      "51/51 [==============================] - 30s 594ms/step - loss: 0.6020 - accuracy: 0.9026\n",
      "Now training efficientnetb4_3\n",
      "Found 3214 images belonging to 6 classes.\n",
      "Classes: [0 1 2 3 4 5]\n",
      "Samples per class: [1177  165  975  113  178  606]\n",
      "Class weights: [1.0, 7.133333333333334, 1.2071794871794872, 10.415929203539823, 6.612359550561798, 1.9422442244224423]\n",
      "Epoch 1/20\n",
      "51/51 [==============================] - 30s 594ms/step - loss: 3.5498 - accuracy: 0.5090\n",
      "Epoch 2/20\n",
      "51/51 [==============================] - 31s 598ms/step - loss: 2.5026 - accuracy: 0.6055\n",
      "Epoch 3/20\n",
      "14/51 [=======>......................] - ETA: 38s - loss: 2.5972 - accuracy: 0.5969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, BatchNormalization, Dropout, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.errors import ResourceExhaustedError\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def calc_class_weights(train_iterator):\n",
    "    \"\"\"\n",
    "    Calculate class weighs dictionary to use as input for the cnn training. This is useful if the training set is\n",
    "    imbalanced.\n",
    "\n",
    "    The weight of class \"i\" is calculated as the number of samples in the most populated class divided by the number of\n",
    "    samples in class i (max_class_frequency / class_frequency).\n",
    "    Note that the class weights are capped at 10. This is done in order to avoid placing too much weight on\n",
    "    small fraction of the dataset. For the same reason, the weight is set to 1 for any class in the training set that\n",
    "    contains fewer than 5 samples.\n",
    "\n",
    "    :param class_counts: A list with the number of files for each class.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Fixed parameters\n",
    "    class_counts = np.unique(train_iterator.classes, return_counts=True)\n",
    "    class_weights = []\n",
    "    max_freq = max(class_counts[1])\n",
    "    class_weights = [max_freq / count for count in class_counts[1]]\n",
    "    \n",
    "    print(\"Classes: \" + str(class_counts[0]))\n",
    "    print(\"Samples per class: \" + str(class_counts[1]))\n",
    "    print(\"Class weights: \" + str(class_weights))\n",
    "\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "def unfreeze_layers(model, last_fixed_layer):\n",
    "    # Retrieve the index of the last fixed layer and add 1 so that it is also set to not trainable\n",
    "    first_trainable = model.layers.index(model.get_layer(last_fixed_layer)) + 1\n",
    "\n",
    "    # Set which layers are trainable.\n",
    "    for layer_idx, layer in enumerate(model.layers):\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = layer_idx >= first_trainable\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(optimiser, last_fixed_layer):\n",
    "    model = EfficientNetB4(include_top=False, input_shape=(300, 400, 3), weights=\"imagenet\")\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = False\n",
    "\n",
    "    # Rebuild top\n",
    "    x = GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = Dense(6, activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "    model = unfreeze_layers(model, last_fixed_layer)\n",
    "    \n",
    "    # Compile\n",
    "    model = Model(model.input, outputs, name=\"EfficientNet\")\n",
    "    model.compile(\n",
    "        optimizer=optimiser, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(rotation, shear, zoom, brightness, lr, last_fixed_layer, batch_size, idx):\n",
    "    model_name = f'efficientnetb4_{idx}'\n",
    "    if os.path.exists(Path('.') / (model_name + '.h5')):\n",
    "        print(f'{model_name} already trained')\n",
    "        return\n",
    "    print(f'Now training {model_name}')\n",
    "    \n",
    "    train_generator = ImageDataGenerator(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rotation_range=rotation,\n",
    "        shear_range=shear,\n",
    "        zoom_range=zoom,\n",
    "        brightness_range=brightness,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=preprocess_input,\n",
    "    )\n",
    "    train_iterator = train_generator.flow_from_directory(\n",
    "        '/home/ubuntu/store/barankin-neurips/full/train', \n",
    "        target_size=(300, 400),\n",
    "        class_mode='categorical',\n",
    "        batch_size=batch_size,\n",
    "        follow_links=True,\n",
    "        interpolation='bilinear',\n",
    "    )\n",
    "    loss_weights = calc_class_weights(train_iterator)\n",
    "\n",
    "    optimiser = Adam(lr=lr)\n",
    "    model = build_model(optimiser, last_fixed_layer)\n",
    "    \n",
    "    logger = CSVLogger(model_name + '.csv')\n",
    "\n",
    "    model.fit(\n",
    "        x=train_iterator,\n",
    "        batch_size=batch_size,\n",
    "        epochs=20,\n",
    "        verbose=True,\n",
    "        class_weight=dict(zip(range(6), loss_weights)),\n",
    "        workers=8,\n",
    "        callbacks=[logger]\n",
    "    )\n",
    "    model.save(model_name + '.h5')\n",
    "\n",
    "\n",
    "for idx in range(5):\n",
    "    train_model(15, 0.5, 0.5, [0.5, 1], 0.001, 'block6d_add', 64, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025cd02",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46b1d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now validating efficientnetb4_4\n",
      "Found 566 images belonging to 6 classes.\n",
      "Now validating efficientnetb4_0\n",
      "Found 566 images belonging to 6 classes.\n",
      "Now validating efficientnetb4_2\n",
      "Found 566 images belonging to 6 classes.\n",
      "Now validating efficientnetb4_3\n",
      "Found 566 images belonging to 6 classes.\n",
      "Now validating efficientnetb4_1\n",
      "Found 566 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.errors import ResourceExhaustedError\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "base_path = \"/home/ubuntu/store/efficientnet-final-size\"\n",
    "model_names = glob.glob(\"/home/ubuntu/store/efficientnet-final-size/*.h5\")\n",
    "\n",
    "for model_path in model_names:\n",
    "    model_name = Path(model_path).stem\n",
    "#     if os.path.exists(Path(base_path) / (model_name + '_preds.csv')):\n",
    "#         print(f'{model_name} already validated')\n",
    "#         continue\n",
    "    print('Now validating', model_name)\n",
    "    valid_generator = ImageDataGenerator(\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function=preprocess_input\n",
    "    )\n",
    "    valid_iterator = valid_generator.flow_from_directory(\n",
    "        '/home/ubuntu/store/DermX-test-set/test', \n",
    "        batch_size=8, \n",
    "        target_size=(300, 400),\n",
    "        class_mode='categorical',\n",
    "        follow_links=True,\n",
    "        interpolation='bilinear',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    model = load_model(Path(model_path))\n",
    "    preds = [np.argmax(pred) for pred in model.predict(valid_iterator)]\n",
    "    actual = valid_iterator.labels\n",
    "    preds_df = pd.DataFrame.from_dict({'actual': actual, 'pred': preds, 'filenames': valid_iterator.filenames}).to_pickle(Path(base_path) / (model_name + '_preds.csv'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce60442",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde0e310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>efficientnetb4_3_preds</th>\n",
       "      <td>0.460444</td>\n",
       "      <td>0.460606</td>\n",
       "      <td>0.436414</td>\n",
       "      <td>566</td>\n",
       "      <td>0.485866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficientnetb4_2_preds</th>\n",
       "      <td>0.450772</td>\n",
       "      <td>0.451591</td>\n",
       "      <td>0.442826</td>\n",
       "      <td>566</td>\n",
       "      <td>0.477032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficientnetb4_1_preds</th>\n",
       "      <td>0.434676</td>\n",
       "      <td>0.456591</td>\n",
       "      <td>0.438718</td>\n",
       "      <td>566</td>\n",
       "      <td>0.482332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficientnetb4_0_preds</th>\n",
       "      <td>0.468710</td>\n",
       "      <td>0.404031</td>\n",
       "      <td>0.382892</td>\n",
       "      <td>566</td>\n",
       "      <td>0.427562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficientnetb4_4_preds</th>\n",
       "      <td>0.373043</td>\n",
       "      <td>0.224015</td>\n",
       "      <td>0.153004</td>\n",
       "      <td>566</td>\n",
       "      <td>0.236749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        precision    recall  f1-score  support  accuracy\n",
       "efficientnetb4_3_preds   0.460444  0.460606  0.436414      566  0.485866\n",
       "efficientnetb4_2_preds   0.450772  0.451591  0.442826      566  0.477032\n",
       "efficientnetb4_1_preds   0.434676  0.456591  0.438718      566  0.482332\n",
       "efficientnetb4_0_preds   0.468710  0.404031  0.382892      566  0.427562\n",
       "efficientnetb4_4_preds   0.373043  0.224015  0.153004      566  0.236749"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "base_path = \"/home/ubuntu/store/efficientnet-final-size\"\n",
    "model_preds = glob.glob(\"/home/ubuntu/store/efficientnet-final-size/*_preds.csv\")\n",
    "model_comparison_dict = {}\n",
    "\n",
    "for model_pred in model_preds:\n",
    "    model_preds_df = pd.read_pickle(Path(model_pred))\n",
    "    model_comparison_dict[Path(model_pred).stem] = classification_report(\n",
    "        model_preds_df['actual'], \n",
    "        model_preds_df['pred'],\n",
    "        labels=[0, 1, 2, 3, 4, 5],\n",
    "        target_names=['acne', 'actinic_keratosis', 'psoriasis_no_pustular', 'seborrheic_dermatitis', 'vitiligo', 'wart'],\n",
    "        output_dict=True\n",
    "    )['macro avg']\n",
    "    model_comparison_dict[Path(model_pred).stem]['accuracy'] = len(model_preds_df[model_preds_df['actual'] == model_preds_df['pred']]) / len(model_preds_df)\n",
    "    \n",
    "model_comparison_df = pd.DataFrame.from_dict(model_comparison_dict, orient='index')\n",
    "model_comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13bc33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_3",
   "language": "python",
   "name": "tf2_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
